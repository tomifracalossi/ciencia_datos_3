# -*- coding: utf-8 -*-
"""mi_modulo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O8h-3E-3JvoXJxWDnJzTbFFN4193yzqR
"""

"""
Este módulo proporciona clases para realizar análisis descriptivo y visualización
de datos, generar datos con cierta distribución, y trabajar con modelos de regresión
lineal y logística

Clases:
    AnalisisDescriptivo
    GeneradoraDeDatos
    Regresion
    RegresionLineal
    RegresionLogistica
"""

#Importemos las librerías
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy.stats import norm, f
from sklearn.metrics import auc
from statsmodels.stats.anova import anova_lm

#-------------------------------------------------------------------------------

class AnalisisDescriptivo:
  """
  Clase para realizar un análisis descriptivo de un conjunto de datos cuantitativos, estimando su
  densidad con un histograma o con kernels (gaussiano, uniforme, cuadrático o triangular)
  A su vez permite graficar el QQ plot con distribución normal estándar, mostrar las estadísticas
  principales y graficar los datos con boxplots

  Atributos de instancia:
      datos: datos cuya densidad se desea estimar
  """

  def __init__(self, datos: np.ndarray):
    """
    Inicializa una instancia de la clase AnalisisDescriptivo
    """
    #Definimos datos como un atributo de instancia
    self.datos = datos

  def estadisticas(self):
    """
    Método para obtener un resumen de las medidas de centralidad y variabilidad principales,
    tal como: media, mediana, desvío, varianza, mínimo, máximo y cuartiles (Q1, Q2, Q3)
    """
    #Crearemos una tabla usando Pandas
    media = np.mean(self.datos)
    mediana = np.median(self.datos)
    desvio = np.std(self.datos, ddof=1)
    varianza = np.var(self.datos, ddof=1)
    minimo = np.min(self.datos)
    maximo = np.max(self.datos)
    Q1, Q2, Q3 = np.percentile(self.datos, [25, 50, 75])
    vector = np.array([media, mediana, desvio, varianza, minimo, maximo, Q1, Q2, Q3])
    diccionario = {'Valor':vector}
    return pd.DataFrame(diccionario, index=['Media', 'Mediana', 'Desvío', 'Varianza', 'Mínimo', 'Máximo', 'Q1', 'Q2', 'Q3'])

  def graficos_matplotlib(self):
    """
    Método para graficar el boxplot y el histograma de los datos cuantitativos usando matplotlib
    """
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.boxplot(self.datos)
    plt.title('Boxplot')
    plt.ylabel('Datos')
    plt.xticks([])
    plt.subplot(1,2,2)
    plt.hist(self.datos, density=True, alpha=0.4, edgecolor='black')
    plt.title('Histograma')
    plt.ylabel('Densidad')
    plt.xlabel('Datos')
    plt.show()

  def __genera_histograma(self, h: float) -> np.ndarray:
    """
    Método privado que genera el histograma de los datos con bandwidth h

    Parámetros:
      h: bandwidth o longitud de cada intervalo o bin
    """
    #Generemos los intervalos
    bins = np.arange(np.min(self.datos),np.max(self.datos)+h,h)
    #Recorremos los intervalos y calculamos la frecuencia absoluta de cada
    #intervalo. Recordemos que tenemos len(bins)-1 intervalos
    absolutas = np.zeros(len(bins)-1)
    for i in range(len(self.datos)):
      for j in range(len(bins)-1):
        if bins[j] <= self.datos[i] < bins[j+1]:
          absolutas[j] += 1
    #Calculo las frecuencias relativas, que es frecuencias absolutas del intervalo
    #sobre la cantidad de datos
    relativas = absolutas / len(self.datos)
    #Calculo la altura de cada intervalo dividiendo por h (pues quiero el histograma
    #en escala de densidad)
    alturas = relativas / h
    #Retorno los intervalos y las alturas
    return bins, alturas

  def estimacion_histograma(self, h: float, x: np.ndarray, retornar=True, graficar=True) -> np.ndarray:
    """
    Método que estima la densidad de los datos sobre una grilla x con un bandwidth h

    Parámetros:
      h: bandwidth o longitud de cada intervalo o bin
      x: grilla de valores para evaluar el histograma
      retornar: valor booleando para determinar si retornar o no las evaluaciones
      graficar: valor booleano para determinar si graficar o no el histograma
    """
    #Utilicemos el método privado __genera_histograma() para obtener los intervalos
    #y las alturas del histograma
    bins, alturas = self.__genera_histograma(h)
    #Veamos en qué intervalo se encuentra cada x. En base a eso le asignamos a
    #cada x su correspondiente altura
    histograma = np.zeros(len(x))
    for i in range(len(x)):
      for j in range(len(bins)-1):
        if bins[j] <= x[i] < bins[j+1]:
          histograma[i] = alturas[j]
    #Podemos retornar las evaluaciones del histograma en la grilla x
    if retornar == True:
      return histograma
    #Podemos graficar
    if graficar == True:
      plt.plot(x, histograma, color='b')
      plt.xlabel('x')
      plt.ylabel('Densidad')
      plt.title('Histograma')
      plt.show()

  #Definimos los kernel como métodos privados
  def __kernel_uniforme(self, x: np.ndarray) -> np.ndarray:
    return (1/2) * ((x > -1) & (x <= 1))
  def __kernel_gaussiano(self, x: np.ndarray) -> np.ndarray:
    return 1/np.sqrt(2*np.pi)*np.exp(-1/2*x**2)
  def __kernel_cuadratico(self, x: np.ndarray) -> np.ndarray:
    return 3/4 * (1-x**2) * ((x > -1) & (x <= 1))
  def __kernel_triangular(self, x: np.ndarray) -> np.ndarray:
    return (1+x)*((x > -1) & (x < 0)) + (1-x)*((x > 0) & (x < 1))

  def estimacion_kernel(self, h: float, x: np.ndarray, kernel='gaussiano', retornar=True, graficar=True) -> np.ndarray:
    """
    Estima la densidad de los datos usando kernels: guassiano, uniforme, triangular y cuadrático

    Parámetros:
      h: bandwidth o longitud de cada intervalo o bin
      x: grilla de valores para evaluar el histograma
      kernel: tipo de kernel para estimar la densidad de los datos
      retornar: valor booleando para determinar si retornar o no las evaluaciones
      graficar: valor booleano para determinar si graficar o no el histograma
    """
    #Defino el tamaño muestral
    n = len(self.datos)
    #Defino un arreglo de frecuencias absolutas de los intervalos
    densidad = np.zeros(len(x))
    #Procedo a definir la función según el kernel
    if kernel == 'uniforme':
      funcion_kernel = self.__kernel_uniforme
    elif kernel == 'gaussiano':
      funcion_kernel = self.__kernel_gaussiano
    elif kernel == 'cuadratico':
      funcion_kernel = self.__kernel_cuadratico
    elif kernel == 'triangular':
      funcion_kernel = self.__kernel_triangular
    else:
      raise ValueError('El kernel está mal especificado')
    #Tomo un x al que le quiero calcular la densidad
    for i in range(len(x)):
      #A todos los elementos del arreglo de datos le resto ese x y lo divido por h
      #Luego aplico la función kernel a todo ese arreglo. Luego sumo todos esos valores
      #Por último divido por n y por h
      densidad[i] = np.sum(funcion_kernel((self.datos - x[i]) / h))/(n*h)
    #Podemos retornar las evaluaciones del histograma
    if retornar == True:
      return densidad
    #Podemos graficar la estimación de densidad usando kernels
    if graficar == True:
      plt.plot(x, densidad, color='b')
      plt.xlabel('x')
      plt.ylabel('Densidad')
      plt.title(f'Estimación con kernel {kernel}')
      plt.show()

  def qqplot_estandar(self, datos=None):
    """
    Método para graficar el QQ plot normal estándar de una muestra de datos

    Parámetros:
      datos: datos que se quiere determinar si provienen de una distribución normal estándar. Pueden ser o no aquellos con los que instancié la clase.
    """
    #Si no le doy datos como parámetro al método, uso aquellos con los que
    #instancié la clase
    if datos is None:
      datos = self.datos
    #Obtengamos los cuantiles muestrales
    n = len(datos)
    media = np.mean(datos)
    desvio = np.std(datos)
    datos = np.sort(datos)
    cuantiles_muestrales = (datos - media) / desvio
    #Obtengamos los cuantiles teóricos de la distribución muestral
    p = np.linspace(1/(n+1), n/(n+1), n)
    cuantiles_teoricos = norm.ppf(p)
    #Graficamos los cuantiles muestrales en el eje x y los cuantiles teóricos en
    #el eje y
    plt.scatter(cuantiles_teoricos, cuantiles_muestrales, color='b')
    plt.plot(cuantiles_teoricos, cuantiles_teoricos, linestyle='-', color='r')
    plt.title('QQ plot')
    plt.xlabel('Cuantiles teóricos')
    plt.ylabel('Cuantiles muestrales')
    plt.show()

#-------------------------------------------------------------------------------

class GeneradoraDeDatos:
  """
  Clase para generar muestras de datos con distribución específica: normal, uniforme,
  t-Student, exponencial y BS

  Atributos de instancia:
    n: tamaño muestral de los datos a generar
  """

  def __init__(self, n: int):
    """
    Inicializa una instancia de la clase AnalisisDescriptivo
    """
    self.n = n

  def muestra_normal(self, mu=0, sigma=1) -> np.ndarray:
    """
    Método para generar datos con distribución normal
    Por defecto la distribución es normal estándar, aunque puede especificarse
    la media mu y el desvío sigma de preferencia

    Parámetros:
      mu: media teórica de la distribución de los datos
      sigma: desvío teórico de la distribución de los datos
    """
    return np.random.normal(mu, sigma, self.n)

  def muestra_uniforme(self, a=0, b=1) -> np.ndarray:
    """
    Método para generar datos con distribución uniforme
    Por defecto el intervalo es [0,1], aunque puede especificarse el intervalo [a,b]

    Parámetros:
      a: limite inferior del intervalo del que provienen los datos
      b: limite superior del intervalo del que provienen los datos
    """
    return np.random.uniform(a, b, self.n)

  def muestra_t(self, df: int) -> np.ndarray:
    """
    Método para generar datos con distribución t-Student
    Deben especificarse los grados de libertad df

    Parámetros:
      df: grados de libertad de la distribución de los datos
    """
    return np.random.standard_t(df, self.n)

  def muestra_exponencial(self, beta) -> np.ndarray:
    """
    Método para generar datos con distribución exponencial

    Parámetros:
      beta: constante beta = 1/lambda de la distribución original de los datos
    """
    return np.random.exponential(beta, self.n)

  def muestra_BS(self) -> np.ndarray:
    """
    Método para generar datos con distribución BS
    """
    #Generamos n datos con distribución uniforme entre 0 y 1
    #O sea me genera probabilidades aleatorias
    u = np.random.uniform(size=(self.n,))
    #Hago una copia independiente de u
    y = u.copy()
    #Tomo el índice del primer elemento de u que sea mayor a 0.5
    ind = np.where(u > 0.5)[0]
    #Actualizo los valores de y donde el elemento es mayor a 0.5 y en su lugar
    #coloco valores de la distribución normal estandar
    y[ind] = np.random.normal(0, 1, size=len(ind))
    #Itero 5 veces
    for j in range(5):
        #Encuentro índices de elementos que cumplen una cierta condición
        ind = np.where((u > j * 0.1) & (u <= (j+1) * 0.1))[0]
        #Asigno esos nuevos valor a y
        y[ind] = np.random.normal(j/2 - 1, 1/10, size=len(ind))
    #Retornamos la muestra con distribución BS
    return y

#-------------------------------------------------------------------------------

class Regresion:
  """
  Clase padre que provee herramientas para realizar análisis de regresión lineal simple
  o múltiple, así como también análisis de regresión logística

  Atributos:
    x: matriz con las variables predictoras en sus columnas
    X: matriz de diseño con una columna de 1 para el intercepto y con las variables predictoras en el resto de columnas
    y: variable respuesta
    results: almacena los resultados del ajuste de regresión
  """

  def __init__(self, X: pd.DataFrame, y: pd.Series):
    """
    Inicializa una instancia de la clase Regresion
    Agrega una columna de 1 a la matriz de diseño
    """

    #Convertimos X en un DataFrame en el caso de que sea un arreglo de NumPy
    if isinstance(X, np.ndarray):
      X = pd.DataFrame(X)
    #Convertimos y en una Serie en el caso de que sea un arreglo de NumPy. Es necesario
    #que tenga una dimensión, por lo que usamos y.ravel()
    if isinstance(y, np.ndarray):
      y = pd.Series(y.ravel())
    #Si X es una Serie, en el caso de tener una única predictora, entonces la convertimos en DataFrame
    #¿Para qué? Para que sm.add_constant funcione adecuadamente
    if isinstance(X, pd.Series):
      X = X.to_frame()

    #Definamos la matriz x sin la columna de 1 (esto será útil cuando trabajemos
    #con regresión lineal simple)
    self.x = X.copy()
    #Definamos la matriz de diseño X añadiendo la columna de 1
    #El parámetro has_constant='add' añade la constante de manera forzosa
    self.X = sm.add_constant(X, has_constant='add')
    #Variable respuesta
    self.y = y
    #Resultados del ajuste inicializados en None
    self.results = None

  def parametros(self):
    """
    Método para retornar los parámetros o coeficientes de regresión estimados del modelo
    """
    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")
    #Retornemos los parámetros
    return self.results.params

  def SE_parametros(self):
    """
    Método para retornar el SE estimado de los coeficientes de regresión estimados del modelo
    """
    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")
    #Retornemos los SE estimados
    return self.results.bse

  def IC_parametros(self, alpha=0.05):
    """
    Método para retornar intervalos de (1-alpha)% de confianza para los parámetros
    o coeficientes de regresión estimados del modelo

    Parámetros:
      alpha: nivel de significancia de los intervalos de confianza (por defecto 0.05)
    """
    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")
    #Retornemos los intervalos de confianza
    return self.results.conf_int(alpha)

  def t_observados_parametros(self):
    """
    Método para retornar los t observados en la muestra asociados a los coeficientes de regresión
    """
    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")
    #Retornemos los t observados
    return self.results.tvalues

  def p_valores_parametros(self):
    """
    Método para retornar los p-valores asociados a los coeficientes de regresión
    estimados del modelo
    """
    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")
    #Retornemos los p-valores
    return self.results.pvalues

  def residuos(self):
    """
    Método para calcular los residuos del modelo lineal
    """
    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")
    #Retornemos los residuos
    return self.results.resid

  def resumen(self):
    """
    Método para mostrar un resumen de los ajustes, parámetros y demás propiedades
    del modelo de regresión ajustado
    """
    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")
    #Mostremos un resumen
    print(self.results.summary())

#-------------------------------------------------------------------------------

class RegresionLineal(Regresion):
  """
  Clase hija que hereda propiedades de la clase Regresion
  También es útil para realizar Analysis of Variance (ANOVA)

  Atributos:
    x: matriz con las variables predictoras en sus columnas
    X: matriz de diseño con una columna de 1 para el intercepto y con las variables predictoras en el resto de columnas
    y: variable respuesta
    results: almacena los resultados del ajuste de regresión
  """

  def __init__(self, X: pd.DataFrame, y: pd.Series):
    """
    Inicializa una instancia de la clase RegresionLineal
    """

    #Hacemos que RegresionLineal herede los atributos de instancia de la clase Regresion,
    #es decir: self.x, self.X, self.y, self.results.
    super().__init__(X, y)

  def ajustar(self):
    """
    Método para ajustar el modelo de regresión lineal a los datos y almacenar los
    resultados en self.results
    """

    #Definimos el modelo con Ordinary Leasts Squares
    modelo = sm.OLS(self.y, self.X)
    #Ajustamos y almacenamos los resultados en el atributo de instancia, actualizando
    #su valor por defecto
    self.results = modelo.fit()

  def analizar_supuestos(self):
    """
    Método para analizar los supuestos del modelo de regresión lineal: errores con
    distribución normal, media 0 y desvío constante sigma^{2}
    """
    #Corroboremos que el modelo fue ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Calculemos la cantidad cant de predictoras (cantidad de columnas de x)
    p = self.x.shape[1]
    #Calculamos la cantidad de observaciones (longitud de la variable respuesta y)
    n = len(self.y)
    #Calculamos la media y el desvío de los errores (este último tiene n-p-1 grados de libertad
    #siendo p la cantidad de predictoras)
    media_error = np.mean(self.results.resid)
    desvio_error = np.std(self.results.resid, ddof=p)
    #Mostramos estos valores
    print(f'La media de los residuos es: {media_error}')
    print(f'El desvío de los residuos es: {desvio_error}')

    #Grafiquemos el QQ plot para determinar si los residuos tienen distribución normal
    #y además un scatter plot de residuos versus valores ajustados
    #Algunos cálculos:
    y_est = self.results.fittedvalues #Valores ajustados
    p = np.linspace(1/(n+1), n/(n+1), n) #Redefinimos p como valores de la forma i/n+1 con 1 <= i <= n
    cuantiles_teoricos = norm.ppf(p, loc=0, scale=desvio_error)
    cuantiles_muestrales = np.sort(self.results.resid)
    plt.figure(figsize=(13,5))

    #Gráfico de los residuos versus valores ajustados
    plt.subplot(1,2,1)
    plt.scatter(y_est, self.results.resid, color='g')
    plt.title('Residuos versus valores ajustados')
    plt.xlabel('Valores ajustados')
    plt.ylabel('Residuos')
    plt.grid(True)

    #QQ plot para analizar si los residuos tienen distribución normal
    plt.subplot(1,2,2)
    plt.scatter(cuantiles_teoricos, cuantiles_muestrales, color='r')
    plt.plot(cuantiles_teoricos, cuantiles_teoricos, label='Recta $y = x$')
    plt.xlabel('Cuantiles teóricos')
    plt.ylabel('Cuantiles muestrales')
    plt.title('QQ plot de los residuos')
    plt.show()

  def prediccion(self, x0: list, alpha=0.05):
    """
    Método para retornar la predicción de la variable respuesta Y para una lista x0
    de valores para las variables predictoras (en orden), así como también el intervalo
    de confianza (para la media condicional) y el intervalo de predicción (para una
    observación puntual)

    Parámetros:
      x0: lista con los valores que toman las variables predictoras
      alpha: nivel de significancia de los intervalos de confianza y predicción (por defecto 0.05)
    """

    #Corroboremos que el modelo fue ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Transformamos x0 en un arreglo de NumPy con 1 fila y tantas columnas como elementos tenga la lista
    #Esto será una matriz de 2 dimensiones (1xp) siendo p la cantidad de predictoras
    x0 = np.array(x0).reshape(1,-1)
    #Agregamos un 1 a la izquierda de la matriz x0, quedando de (1x(p+1))
    #Si sm.add_constant() detecta que ya está la columna de 1, no la agrega. Por ello lo forzamos
    #haciendo has_constant='add'
    x0 = sm.add_constant(x0, has_constant='add')

    #Realizamos la predicción y retornamos un resumen con la predicción, el intervalo de confianza
    #y el intervalo de predicción
    #Internamente se realiza el producto punto de los p+1 coeficientes de regresión con los p+1 coeficientes de x0
    return self.results.get_prediction(x0).summary_frame(alpha)


  def ANOVA(self, manual=False, alpha=0.05):
    """
    Método para realizar el test ANOVA
    Nuestro modelo es Y_ij = mu_i + epsilon_ij (mu_i factor del grupo i)
    con epsilon_ij normal con media 0 y desvío constante
    Además suponemos independencia de las observaciones entre y dentro de los grupos
    H0: mu_i = mu_j para todo i,j
    H1: mu_i distinto de mu_j para i distinto de j

    Si tenemos I grupos, traducimos esto a un modelo de Regresión Lineal de la forma 
    Y = beta0 + beta1 X1 + ... + betaI-1 XI-1 + epsilon
    Notemos que mu_i = beta0 + betai para i entre 1 e I-1
    A partir del modelo de Regresión Lineal las hipótesis son
    H0: betas = 0
    H1: algún beta distinto de 0

    Considerando el modelo bajo H0, dado por Y = beta0 + epsilon
    realizo un Test ANOVA para determinar con cual modelo quedarme
    Ahora las hipótesis son
    H0: modelo reducido
    H1: modelo completo

    El modelo reducido equivale a afirmar que las medias son iguales
    El modelo completo equivale a afirmar que las medias son distintas
    Decidiremos sobre el modelo en base al p-valor obtenido del test ANOVA

    Se recuerda que es necesario corroborar los supuestos del modelo
    """
    #Corroboremos que el modelo fue ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Construyamos la matriz de diseño del modelo reducido bajo H0
    X_red = np.ones(len(self.y))
    #Ahora ajustemos el modelo reducido
    modelo = sm.OLS(self.y, X_red)
    modelo_reducido = modelo.fit()
    #Cambiemos el nombre de un atributo de instancia de forma temporal
    modelo_completo = self.results

    #Cuentas a mano
    if manual == True:
      #Suma de los residuos al cuadrado
      RSS_M = np.sum(modelo_completo.resid**2)
      RSS_m = np.sum(modelo_reducido.resid**2)
      #Calculemos la cantidad de datos
      n = len(self.y)
      #Calculo los grados de libertad de cada modelo en función de la cantidad de parámetros
      #beta que tiene cada uno
      gl_M = n-self.X.shape[1] #beta0, beta1, ..., betaI-1
      gl_m = n-1 #beta0
      #Calculo el numerador y el denominador del F observado, o sea el S_F^{2} y el S_p^{2}
      #de nuestros datos
      numerador = (RSS_m - RSS_M) / (gl_m - gl_M) #S_F^2
      denominador = RSS_M / gl_M #S_p^2
      #Calculo el F observado, o sea S_F^2 / S_p^2
      F_obs = numerador / denominador
      #Imprimo el numerador, el denominador y el F_obs
      print(f'Numerador S_F^2 = {numerador}')
      print(f'Denominador S_p^2 = {denominador}')
      print(f'F observado en nuestros datos, o sea S_F^2 / S_p^2 = {F_obs}')
      #Denotaremos con dn los grados de libertad del numerador
      dn = gl_m - gl_M
      #Denotaremos con dd los grados de libertad del denominador
      dd = gl_M
      #Calculemos el p-valor como la probabilidad de hallar un cociente mayor o igual
      #al F_observado
      p_value = 1 - f.cdf(F_obs, dn, dd)
      #Mostremos dicho p-valor
      print(f'El p-valor es {p_value}')

    #Cuentas utilizando de la función anova_lm de Scipy:
    if manual == False:
      return anova_lm(modelo_reducido, modelo_completo)

  def R2(self, ajustado: bool = False):
    """
    Método para calcular el R^{2} y el R^{2} ajustado del modelo

    Parámetros:
      ajustado: valor booleano que determina si queremos el R^{2} ajustado o no
    """
    #Corroboramos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Si queremos que R^{2} ajustado, lo retornamos
    if ajustado == True:
      return self.results.rsquared
    #Si no, retornamos el R^{2}
    else:
      return self.results.rsquared_adj

  def correlacion(self):
    """
    Método para calcular el coeficiente de correlación de Pearson entre la variable
    predictora y la variable respuesta, suponiendo que tenemos una única predictora
    """
    #Corroboramos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Si tenemos una única variable predictora
    if self.x.shape[1] == 1:
      #Calculamos el coeficiente de correlación y lo retornamos
      #Recordemos que x es una matriz, pues al ajustar lo transformamos en DataFrame
      return np.corrcoef(self.x.iloc[:,0], self.y)[0,1]
    #Si tengo más de una predictora calculo el coeficiente de correlación para cada una contra la respuesta
    elif self.x.shape[1] > 1:
      #Defino un vector vacío
      vector = np.zeros(self.x.shape[1])
      #Recorro las predictoras
      for i in range(self.x.shape[1]):
        #Calculo el coeficiente y lo almaceno en el vector
        vector[i] = np.corrcoef(self.x.iloc[:,i], self.y)[0,1]
      #Retorno el vector con los coeficientes
      print('Correlación entre cada predictora y la respuesta:')
      return vector

  def graficar(self):
    """
    Método para graficar el scatter plot de los datos junto a la recta de regresión estimada,
    suponiendo que estamos trabajando con una única variable predictora
    En el caso de trabajar con varias predictoras, graficaremos únicamente el scatter plot
    de cada variable predictora contra la variable respuesta
    """

    #Corroboramos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Si tenemos una única variable predictora
    if self.x.shape[1] == 1:

      #Calculemos las estimaciones óptimas de los coeficientes de regresión
      b0, b1 = self.results.params
      #Mostremos un string
      print(f'La ecuación de la recta es y = {b0} + {b1} * x1')

      #Definamos una grilla de valores x para graficar
      x = np.linspace(np.min(self.x), np.max(self.x), 1000)
      #Calculamos las imágenes de esta recta en la grilla de valores x
      y = b0 + b1 * x

      #Grafiquemos
      plt.scatter(self.x, self.y, color='b', label='Datos') #Gráfico de dispersión de X e Y
      plt.plot(x, y, color='r', label='Recta de regresión estimada') #Recta estimada de regresión
      plt.xlabel('X')
      plt.ylabel('Y')
      plt.title('Regresión Lineal Simple')
      plt.legend()
      plt.grid(True)
      plt.show()

    #Si tenemos varias predictoras
    elif self.x.shape[1] > 1:

      #Calculamos el número de predictoras
      p = self.x.shape[1]

      #Recorremos cada variable predictora y hacemos el scatter plot contra la respuesta
      for i in range(p):
            plt.scatter(self.x.iloc[:, i], self.y, color='b', label=f'Predictora {i+1}')
            plt.xlabel(f'Predictora {i+1}')
            plt.ylabel('Respuesta')
            plt.title('Scatter plot')
            plt.show()

#-------------------------------------------------------------------------------

class RegresionLogistica(Regresion):
  """
  Clase hija que hereda propiedades de la clase Regresion
  ATENCIÓN: será necesario pasarle como entrada las predictoras y la respuesta de datos_train

  Atributos:
    X_train: matriz de diseño de datos_train (en sus columnas tiene las variables predictoras numéricas) sin incluir la columna de 1
    y_train: variable respuesta de datos_train
  """

  def __init__(self, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.DataFrame):
    """
    Inicializa una instancia de la clase RegresionLogistica
    """

    #Hacemos que RegresionLogistica herede los atributos de instancia de la clase Regresion,
    #es decir: self.x, self.X, self.y, self.results. Notemos que añade automáticamente una columna
    #de unos a la matriz de diseño según la clase Regresion. Además self.results = None por defecto
    super().__init__(X_train, y_train)

    #De ahora en más self.X y self.y corresponderán a las predictoras X_train e y_train

    #Definimos los atributos de instancia adicionales
    self.X_test = X_test
    self.y_test = y_test

  def ajustar(self):
    """
    Método para ajustar el modelo de regresión logística a los datos y almacenar los
    resultados en self.results
    """

    #Agreguemos constantes a las matrices de diseño de datos_test, actualizando sus valores
    #A la de datos_train no es necesario pues lo hace la clase padre
    self.X_test = sm.add_constant(self.X_test)

    #Mostremos un resumen de los tamaños
    print('Tamaños muestrales:')
    print(f'datos_train: {self.X.shape}')
    print(f'datos_test: {self.X_test.shape}')

    #Definimos el modelo de regresión logística con los datos de entrenamiento, almacenados en self.X y self.y
    modelo = sm.Logit(self.y, self.X)
    #Ajustamos y almacenamos los resultados en el atributo de instancia, actualizando
    #su valor por defecto
    self.results = modelo.fit()

  def prediccion(self, x0: list):
    """
    Método para retornar la predicción de la variable respuesta Y para una lista x0
    de valores para las variables predictoras (en orden

    Parámetros:
      x0: lista con los valores que toman las variables predictoras
    """

    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Transformamos x0 en un arreglo de NumPy con 1 fila y tantas columnas como elementos tenga la lista
    #Esto será una matriz de 2 dimensiones (1xp) siendo p la cantidad de predictoras
    x0 = np.array(x0).reshape(1,-1)
    #Agregamos un 1 a la izquierda de la matriz x0, quedando de (1x(p+1))
    #Si sm.add_constant() detecta que ya está la columna de 1, no la agrega. Por ello lo forzamos
    #haciendo has_constant='add'
    x0 = sm.add_constant(x0, has_constant='add')

    #Realizamos la predicción y la retornamos
    return self.results.predict(x0)[0]

  def matriz_confusion(self, p: float = 0.5, medidas: bool = False, imprimir_tabla: bool = True):
    """
    Método para construir la matriz de confusión y así analizar la bondad del modelo
    Estamos suponiendo que el modelo se ajustó con datos_train y ahora utilizaremos
    datos_test para verificar su buen funcionamiento

    Parámetros:
      p: punto de corte particular (por defecto 0.5)
      medidas: valor booleano que permite retornar o no la sensibilidad y especificidad del modelo
      imprimir_tabla: valor booleano que permite imprimir o no la matriz de confusión
    """

    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Predecimos las probabilidades para datos_test utilizando el modelo con los
    #datos de entrenamiento, almacenado en results
    predicciones = self.results.predict(self.X_test)
    #Definamos la variable y_pred que vale 1 si predicciones >= p y 0 sino
    y_pred = 1 * (predicciones >= p)
    #Corroboremos
    if len(y_pred) != len(self.y_test):
      raise ValueError('Las variables y_test e y_pred tienen distinta longitud')

    #Construyamos la matriz de confusión
    a  = np.sum((y_pred == 1) & (self.y_test == 1))
    b = np.sum((y_pred == 1) & (self.y_test == 0))
    c = np.sum((y_pred == 0) & (self.y_test == 1))
    d = np.sum((y_pred == 0) & (self.y_test == 0))
    #Definamos un diccionario
    diccionario = {'y_test = 1':np.array([a, c, a+c]), 'y_test = 0':np.array([b, d, b+d]), 'Total':np.array([a+b, c+d, a+b+c+d])}
    #Creamos una tabla de Pandas y mostrémosla
    tabla = pd.DataFrame(diccionario, index=['y_pred = 1', 'y_pred = 0', 'Total'])

    #Si pedimos que imprima la tabla, lo hacemos, junto al error de mala clasificación
    if imprimir_tabla == True:
      print(tabla)
      print(f'Error de mala clasificación: {(b+c) / len(self.y_test)}')
    #Si pedimos que retorne las medidas de sensibilidad y especificidad, lo hacemos
    if medidas == True:
      sensibilidad = a / (a+c)
      especificidad = d / (b+d)
      return [float(sensibilidad), float(especificidad)]

  def corte_optimo(self, n: int = 100, graficos: bool = True):
    """
    Método para calcular el punto de corte óptimo con el índice de Jouden, así como
    graficar curvas y medidas que lo confirmen

    Parámetros:
      n: cantidad de puntos de corte a construir entre 0 y 1 (por defecto 100)
      graficos: valor booleano para mostrar el gráfico de sensibilidad y ajuste versus p, y la curva ROC
    """

    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Calculamos los n puntos de corte entre 0 y 100
    p = np.linspace(0, 1, n)
    #Definimos arreglos para almacenar la sensibilidad y especificidad
    sensibilidad = np.zeros(n)
    especificidad = np.zeros(n)

    #Recorremos cada p con un ciclo for
    for i in range(len(p)):
      #Usando el método anterior calculo la sensibilidad y especificidad
      s, e = self.matriz_confusion(p[i], True, False)
      #Almaceno esos valores en los arreglos
      sensibilidad[i] = s
      especificidad[i] = e

    #Calculemos el p óptimo usando el índice de Jouden que maximiza la siguiente expresión
    expresion = sensibilidad + especificidad - 1
    #Llamaremos a este índice: índice de Jouden
    J = np.argmax(expresion)
    #Calculemos el p que tiene dicho índice
    p_buscado = p[J]
    #Mostremos los resultados
    print(f'El p buscado es {p_buscado}')
    print(f'La sensibilidad correspondiente es {sensibilidad[J]}')
    print(f'La especificidad correspondiente es {especificidad[J]}')

    #Calculemos el área bajo la curva ROC usando el índice de Jouden
    #Nos dará una idea de qué tan bueno es nuestro modelo creado con datos_train
    area = auc(1-especificidad, sensibilidad)
    print("AUC:", area)
    #En base al área calculada, especifiquemos qué tan bueno es nuestro modelo
    if 0.9 <= area <= 1:
      print('Rango de AUC: excelente')
    elif 0.8 <= area < 0.9:
      print('Rango de AUC: bueno')
    elif 0.7 <= area < 0.8:
      print('Rango de AUC: regular')
    elif 0.6 <= area < 0.7:
      print('Rango de AUC: pobre')
    elif 0.5 <= area < 0.6:
      print('Rango de AUC: fallido')

    #Si pido que grafique, lo hago
    if graficos == True:
      plt.figure(figsize=(15,5))
      #Grafiquemos la sensibilidad y la especificidad a medida que aumenta p
      plt.subplot(1,2,1)
      plt.plot(p, sensibilidad, 'b', label='Sensibilidad')
      plt.plot(p, especificidad, 'g', label='Especificidad')
      plt.title('Sensibilidad y especificidad en función de p')
      plt.xlabel('$p$')
      plt.ylabel('Valor')
      plt.grid(True)
      plt.legend()
      #Grafiquemos la curva ROC y el punto óptimo
      plt.subplot(1,2,2)
      plt.plot(1-especificidad, sensibilidad, 'r')
      plt.plot(1-especificidad[J], sensibilidad[J], 'o', color='b', label='Punto óptimo')
      plt.title('Receiver Operating Characteristic')
      plt.xlabel('$1-$Especificidad')
      plt.ylabel('Sensibilidad')
      plt.grid(True)
      plt.legend()
      #Mostremos los gráficos
      plt.show()

  def graficar(self):
    """
    Método para graficar la variable predictora numérica versus la variable respuesta binaria
    y ver la curva sigmoide que la aproxima
    """

    #Corroboremos que el modelo haya sido ajustado
    if self.results is None:
      raise ValueError("El modelo aún no ha sido ajustado. Ejecute el método 'ajustar()' primero.")

    #Corroboremos que tenemos una única variable predictora
    #Recordemos que, según la clase padre, self.x almacena la matriz de datos_train sin la columna constante
    if self.x.shape[1] == 1:
      #En cuyo caso calculamos los coeficientes de regresión del log-odds del modelo con datos_train
      b0, b1 = self.results.params
      #Creamos un rango de valores para la predictora para graficar la curva sigmoide
      #Tomo el mínimo y el máximo con min() y max() y luego, como x es matriz, le tomo el elemento
      #correspondiente con iloc[0]
      a = self.x.min().iloc[0]
      b = self.x.max().iloc[0]
      x_rango = np.linspace(a, b, 1000)
      #Defino en una variable auxiliar la recta
      aux = b0 + b1 * x_rango
      #Calculo las predicciones de probabilidad con la sigmoide
      probabilidades = 1 / (1 + np.exp(-aux))
      #Graficamos los datos de entrenamiento y la curva sigmoide
      plt.figure(figsize=(10, 6))
      plt.scatter(self.X.iloc[:, 1], self.y, color='blue', label='Datos de Entrenamiento', alpha=0.6)
      plt.plot(x_rango, probabilidades, color='red', label='Curva Sigmoide Estimada')
      plt.xlabel(self.x.columns[0])
      plt.ylabel('Probabilidad de Y = 1')
      plt.title('Regresión Logística con Curva Sigmoide')
      plt.legend()
      plt.grid(True)
      plt.show()

    #De lo contrario, error
    else:
      raise ValueError('Este método solo puede aplicarse con una única variable predictora en el modelo')